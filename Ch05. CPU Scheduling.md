- CPU 스케쥴링 : CPU를 프로세스 도중 스위치해가면서, OS가 CPU를 최대한 잘 많이 사용하기 위해
- 중간고사까지는 프로세스 스케쥴링만 다룸
- CPU에서 프로세스를 스케쥴링 한다

- CPU 사용률을 최대화!
	- 한 순간에 여러 프로세스를 메모리에 유지
	- 프로세스가 wait 할 때마다, OS가 CPU를 다른 프로세스에게 넘겨줌
- 멀티코어 시스템에서는 ,CPU를 busy로 유지하는걸 모든 코어로 확장

- 프로세스 실행은 CPU 실행(burst) + I/O 대기(burst의 사이클임
- CPU burst -> I/O burst - > CPU burst -> I/O ->.. CPU birt로 반복
- CPU가 I/O보단 하나 많겟군

- I/O 바운드 프로그램은 CPU 버스트가 많은대신 짧을거고
- CPU 바운드 프로그램은 길고 적은 CPU 버스트가 잇겟지
- 그래서 짧은 CPU 버스트가 대부분이고 긴건 일부임
- ![](https://i.imgur.com/SPGCsxX.png)
그래서 이런 형태


- CPU가 idle 상태가 될 떄마다, OS가 레디큐에서 한 프로세스를 골라서 실행 (cPU가 안 놀게)
- CPU 스케쥴러는 레디가 된 메모리 내 프로세스부터 골라서 CPU 할당
- 레디큐의 모든 프로세스는 CPU에서 실행되기를 기다리며 대기
- 큐의 기록은 PCB에 linked로 구현

- 스케쥴링 시점에서, 다음의 4가지 상태
	![](https://i.imgur.com/qHsA6Ax.png)
	- 1) 프로세스가 running->waiting으로 갈대
	- 2) 프로세스가 running->ready로 갈때(인터럽트!)
	- 3) 프로세스가 waiting->ready로 갈때 (I/O 완료)
	- 4) 프로세스가 종료될떄
	- 선택의 여지 없이 큐에서 쭉쭉 뽑아오면 비선점
	- 선택의 여지가 있으면 (계속할까? 아님 새거 가져올까?) 선점
	- 선점 허용시 고려해야 할 것
		- 여러 프로세스 간 데이터 공유 시 경쟁조건 초래
			- 데이터 일관성을 잘 잡아줘야 함
		- 커널 설계에 영향
			- 시스템콜 동안, 커널은 프로세스를 위한 활동으로 바쁨
			- 프로세스가 커널 데이터처리 중 선점되고, 커널이 그걸 사용하면 혼돈

- OS 커널의 설계
	- 비선점이면 : 기다리고 시스템콜이 완료될때까지 기다렸다가, IO완료까지 블록되기를 기다림
	- 선점이면 : 공유 커널 자료에 접근 시 경쟁조건 방지 위한 메커니즘 필요
- 인터럽트
	- 인터럽트는 언제든 발생하고 커널에서 무시를 못함
	- 따라서 인터럽트 영향받는 코드 부분은 항상 동시사용을 못하게 보호해야함
- 디스패쳐
	- 스케쥴러가 단순히 레디큐에서 하나 선택하는것까지라면
	- 얘는 그 나머지 작업임 (선택된 프로세스에게 CPU코어의 제어를 넘김)
	- 레이턴시가 있음. 디스패처가 한 프로세스를 멈추고 다른 프로세스를 실행하기까지의 시간!
	- context switch의 빈도 : vmstat 명령어 치면 cs에 1초당 얼마나 일어나는지 나옴
	- 자발적/비자발적 context switch가 있음
		- 자발적 : 현재 사용 불가능한 자원 요청
		- 비자발적 : CPU를 뺏김


# 5.2 Scheduling Criteria

- 스케줄링 알고리즘 별로 특성이 다르고, 비교 기준은 많음
	- CPU 사용률 , Throughput(처리량), turnaround (처리시간)
	- waiting time, response time등이 있음 (사처처대응)
- 보통 평균을 때릴수도 있는데..
- 편의를 위해 rpocess 당 하나의 CPU burst만 고려하고, 알고리즘 비교 기준은 avg 대기시간

- 레디큐 내의 어떤 프로세스에게 CPU 코어를 할당할 것인가의 문제
- 한번에 하나만 실행된다고 생각하자

# 5.3 스케쥴링 알고리즘 (한문제나옴!!)

## 5.3.1 FCFS (선착순)

- 처음 요청한 프로세스가 CPU를 처음으로 할당 받음
- 프로세스가 레디큐에들어가면, PCB가 큐의 끝에 연결
- CPU가 사용가능해지면 큐의 맨앞에 있는 process에 할당
- 단점) 평균 대기시간이 길어질 수있음
	- 최소를 보장 못하므로, CPU burst가 크게 다르다면 평균대기시간이 크게 달라짐
	- Convoy effect (호위효과)
		- 긴놈 뒤에 짧은놈들이 다 긴놈 기다림
		- 짧은놈이 먼저 실행되는것보다 여러모로 줄어듬
	- 선점을 못하니까... 문제네..

## 5.3.2 SJF (짧은놈 먼저)

- 매번 CPU burst가 최소인 애한테 CPU가 할당되고, 동일하다면 선착순 적용
- next CPU burst의 길이가 중요!
- 항상 avg 대기시간이 최소임이 중요하지만
- 어떻게 next CPU를 알 건데~ 이건 CPU 스케쥴링 수준에선 구현 못 하지~~~~~
- 해결법) approximate SJF 적용
	- next CPU burst를 예측하다
	- 이전의 값과 비슷할 것으로 예측하자
	- 측정한 이전 CPU burst의 길이를 가중평균합시다
	- 이ㄹ걸 계산해서 가장 작은놈을 고릅시다
	- $$\tau_{n+1} = \alpha t_{n}+ (1-\alpha)\tau_n$$
	- $\tau_{n+1}$ : next CPU burst 예측값
	- $t_n$ : n번째 CPU burst r길이 (가장 최근의 정보인거죵)
	- $\tau_n$ : 과거
	- 이렇게 $t_{0},\tau_{0}$ 부터 시작하는거죠
	- $\alpha$ 가중치를 어떻게 둘거냐에 따라, 얘기가 달라지겠죠
	- 보통 alpha랑 $\tau_0,t_0$ 은 주어집니다 문제에서!
	- 점점 $\tau_n,t_n$은 비슷하게 수렴합니다
	- 보면 알겠지만 가장 최근 t가 가중치가 크게 들어갈거에요

- SJF는 선점/비선점 방식 모두 가능함
	- 이전 프로세스가 실행되는 동안, 새 프로세스가 레디큐에 오면 선택함
	- 새로온놈의 버스트 < 지금 하고있는거의 남아있는 burst 면 체인지!!!
	- 언제 vs를 붙이면 새로 들어온 놈이 생길 때마다 (새로 온놈이 도착할때)

- nompreemtive면 어떻게 하나요? 그럼 일단 한번 실행되고 있는 애는 안뺏기고 계속 가요


## 5.3.3 RR(Round-Robin) 

- FCFS(선착순)에 선점을 추가한거
- time quantam이라는 작은 시간 단위를 적용하고,
- 일단 선착순으로 하되,
	- CPU burst가 1 tq 안에 끝나면 (끝났다) CPU 스스로 해제
	- 다음 프로세스~
	- CPU가 1tq보다 길면,
	- ㅂ강제로 바꿔버림. 실행중이던 애를 레디큐 끝자락, 다음놈 오세요
	- 문제에선 tq를사전에 정해줄거에요
	- avg 웨이팅 타임이 길어질 순 없음
	- 2tq를 연속으로 할당받는 놈은 혼자 남아있지 않는 한없음
	- 프로세스가 n개면, 각 프로세스는 1/n 씩 시간 먹고, 최대 q의tq를 먹음
	- (n-1) x q tq만큼 대기하게됨
	- tq로 선점을 구현한건데, TQ가 너무 길어지면 FCFS랑 똑같아짐
	- 엄청 짧아지면 context switch가 너무 많이 일어나서 문제임
	- 적당한 놈을 잡으라고

- context switch time < TQ 기를 바람
- 문맥이 TQ의 10%면, CPU시간의 10%를 문맥에 사용
- turnaround (처리시간-도착부터 시작이 아닌 도착부터 종료까지 시간) 역시 TQ의 크기에 좌우됨
	- 대부분의 프로세스가 1 TQ 안에 끝나면 총 처리시간은 향상됨
	- 문맥 교환 시간이 추가되면 깝깝하죠 
	- TQ가 커진다고 해서 avg 턴어라운드가 향상되는 건 아님
- 경험적으로 CPU버스트의 80%는 시간 할당량보다 짧도록 ,TQ를 설정


### 5.3.4 Priority Scheduling(PCB에 우선순위 설정)

- 낮은 번호가 우선순위가 높다고 생각핪디ㅏ
- 우선순위가 같음 선착순(FCFS
- 생각해보면 SJF는 우선순위를 next CPu burst의 반비례로 설정한 거네

- 우선순위는 내외부적으로 정할 수 있는데
	- 내부  : 시간제한, 메모리요구 ,사용파일수, IOvscCPU 비율
	- 외부 : OS외부에서 설정 (프로세스 중요도, 비용, 부서, 정치적 요인)

- 이것도 선점/비선점으로 할 수 있음
- 프로세스가 레디큐에 오면, 실행 중인 프로세스와 우선순위를 비교해
- 선점 : 새로 도착한애 우선순위 > 현재 실행된애 우선순위 면 체인지
- 비선점 : 새로 도착한애가 레디큐 맨앞에

- 뭐가 문제임?
	- 실행될 준비가 되었으나 CPU 대기중인 프로세스는 block으로 간주
	- 우선순위가 낮은 애들은 무한 대기할 수 있음
	- 부하가 큰데 우선순위 높은애들만 계속 들어오는 경우
	- 이럼 두가지경우지
		- 부하가 낮아졌을때 다행히 실행되거나
		- 시스템이 crash되어서 우선순위 낮은 애들은 날아가는거임

- 이걸 방지하려면?
	- Aging : 오래 있을수록 프로세스의 우선순위를 점차 높여준다
	- RR+우선순위 : 가장 우선순위가 높은애를 실행하되, 동일하면 TQ를 돌려막기한다

<hr style="page-break-after: always;">

# 기말고사 범위 시작

## 5.3.4 Multi-level Queue scheduling

- Priority와 Round robin의 경우를 떠올려 봅시다..
- 모든 프로세스는 **하나의 큐**에 자리하고, 스케쥴러가 가장 높은 우선순위를 선택해서 실행하죠?
- Queue가 어떻게 관리되냐에 따라 달라지긴 하지만,
	- 가장 높은 우선순위를 찾는데 $O(n)$ 이나 소모해버림.. ㅠ

- **해결책 : 우선순위마다 다른 큐를 운영하기(Multi-level Queue Scheduling)**!
	- 단순히 '우선순위'가 가장 높은 큐에 있는 프로세스를 스케줄합시다
	- 만약 그 **큐에 여러 개의 프로세스가 존재한다면? Round Robin**을 적용하구!
	- 이 때 **우선순위는 정적으로 할당**됩니다
	- 프로세스는 **실행되는 동안에는 항상 같은 큐**에 남아 있을 거에요
	- **Process를 타입에 따라 개별 큐로 분할하는 방법**도 있어요 (사용자가 신경을 쓰는 곳일수록 우선순위 高)
		![](https://i.imgur.com/HW9BPAe.png)
	- **게다가, 큐 간에 스케쥴링 알고리즘이 적용되기도!**
		1. **Fixed-priority preemptive echeduling**
			- 큐 별로 우선순위는 절대적이고
			- 낮은 우선순위의 큐에 있는 프로세스는, 상위 큐가 비지 않으면 실행 절대 안됨
			- 낮은 큐에서 실행 중일 때, 높은 큐에 프로세스가 돌아오면 선점됨
		2. **큐 별로 time slice가 부여될수도 있음**
			- 각 큐는 CPU time의 일부를 할당받고, 그걸로 큐 내의 프로세스를 스케쥴링해여
			- 중요한 큐는 높은 비율을 주고, 큐 내에서 적용되는 스케쥴링 알고리즘은 큐별로 다를수도~

- 프로세스가 시스템에 진입하면, **하나의 큐에 영구적으로 할당**
- **큐 간에 절대로 옮겨다니지 않음** (중요한 큐는 중요한 큐인 이유가 있지)
- **장점) 스케쥴링 오버헤드 적게 발생** 
- **단점) 유연한 대응못함ㅜ**
## 5.3.6 Multi-level 'Feedback' Queue Scheduling (변형된 버전)

**Fixed-priority preemptive echeduling을 쓰는 건데 .. **
- 프로세스가 큐를 옮겨 다닐 수 잇도록
- CPU burst가 짧을수록 높은 우선순위 큐에
- Aging : Starvation 방지로, 낮은 큐에서 너무 오래 대기하면 조금씩 높은 큐로 보냄.

## HRN(Highest Response-ratio Next)

- FIFO(FCFS)와 SJF의 단점 보완
	- FIFO는 최적화 기능이 없고, SJF는 실행 시간이 긴 건 불리함
	- Preemptive SJF 시, 남은 시간을 계산하고 저장해야 해서 구현 빡셈
- 실행 시간과 대기시간에 따라 우선순위 결정!
	$$\frac{\text{대기시간+실행시간}}{\text{실행시간}}\text{이 큰 값부터 실행}$$
<hr style=\"page-break-after: always;\">


# 5.4 Thread Scheduling

- 현대 OS에서는 프로세스가 아니라 "커널 레벨의 쓰레드"가 스케쥴링 대상임
- 유저 레벨 쓰레드는 thread library에 의해 관리됨, 커널은 관심 없음
	- CPU에서 실행되려면, 관련된 커널레벨 쓰레드에 맵핑되어야 함
- 유저 레벨 쓰레드 vs 커널 레벨 쓰레드의 대표적 구분 : 스케쥴 방식에서의 차이!
	- **유저 레벨 쓰레드 : Process contention scope (PCS)**
		- 한 프로세스 내 쓰레드 간 LWP 경쟁 발생
		- 다대일 / 다대다 모델 : 쓰레드 라이브러리가 유저레벨 쓰레드가 가용한 LWP에서 실행되도록 스케쥴
			- 이게 곧 **LWP의 커널 쓰레드를 물리적 CPU 코어에 스케쥴링 하는 것!**
		- 우선순위가 높은 순대로
		- 실행중인 쓰레드 선점도 가능
	- **커널 레벨 쓰레드 : System contention scope (SCS)**
		- 시스템 전체 쓰레드 간 CPU 코어 경쟁 발생해서
		- 어떤 커널레벨쓰레드가 CPU에서 스케쥴될지 정함
- **일대일 모델**(윈도, 리눅스)의 경우 **SCS**만 사용함

## 5.4.2 Pthread Scheduling

- Pthread는 다음의 scope value를 식별한다
- $\texttt{PTHREAD\_SCOPE\_PROCESS}$ : PCS 스케줄링
	- 유저 레벨 스레드를 가용한 LWP에 스케쥴
	- LWP 개수는 스레드 라이브러리가 유지시킴
- $\texttt{PTHREAD\_SCOPE\_SYSTEM}$ : SCS 스케쥴링
	- 각 유저 스레드에 대한 LWP를 생성하고 연결함. LWP-코어 일대일 매핑
<hr style=\"page-break-after: always;\">


# 5.5 Multi-Processor Scheduling

(프로세스 1개-스레드-여러개 확장)

- 여러 CPU가 가용하다면, 부하를 공유할 수 있음
	- 여러 스레드가 병렬로 동작 가능!
	- 스케쥴링이 훨씬 복잡해지긴 하겠다..

- (전통적) Multiprocessor : 여러 물리 프로세서를 제공하는 시스템. 프로세스당 한개의 싱글코어 cpu 포함
- (현대적) Multiprocessor
	- Multicore CPU
	- Multithread CPU
	- NUMA (각 CPU에 로컬버스로 접근가능한 자신만의 메모리 제공, 모든 CPU가 공유시스템 연결, 물리주소공간 공유)
		![|400](https://i.imgur.com/LF7aafo.png)

	- 위에 둔 3개는 프로세서들이 기능적으로 동질함.
	- 큐에 있는 프로세스를 실행하는 데에 아무 가용한 CPU나 써도 됨
	- Heterogeneous multiprocesing
		- 이건 프로세서들이 기능적으로 동질하지 않음..

## 5.5.1 Multiple-Processor Scheduling 방법론

- **Multiprocessor 시스템에서의 CPU 스케줄링**
	- **Asymmetric(기능적으로 다른) Multiprocessing (AMP)**
		- 모든 스케쥴링 결정, I/O 프로세싱, 기타등등 활동 다 하나의 프로세서 **(마스터서버)** 가 처리
		- 나머지는 유저 코드만 실행
		- 장) 간단, 한 코어만 시스템 자료구조 접근, 데이터 공유 배제
		- 단) 마스터 서버가 시스템 전반 성능을 낮추는 보틀넥이 될 가능성
	- **Symmetric (기능적으로 동일) Multiprocessing (SMP)**
		- Multiprocessor 지원하는 일반적 방법
		- 프로세서 별로 스케줄링 가능
			- 프로세서 별 스케줄러가 레디큐 확인, 실행할 스레드 선택함
		- **스레드 관리 방법**
			![](https://i.imgur.com/wR1R3Nw.png)

			- **모든 스레드가 하나의 공통 레디 큐 사용**
				- Race Condition 발생: 동일 스레드 스케쥴 X, 큐에서 스레드 없어지지 않도록 보장
			- **각 프로세서가 자기만의 개별 레디 큐 사용**
				- 가장 흔한 방법
				- 코어 내 캐시 메모리 데이터를 효율적으로 사용
				- 큐마다 부하의 양이 다르겠지
				- 모든 프로세서 간 부하를 균등하게 하는 알고리즘 사용

## 5.5.2 Multicore Processors

- SMP 시스템은 여러 프로세스들이 병렬로 실행되게 함
- **Multicore Processor**
	- 여러 코어가 하나의 칩에 있음
	- 각 코어는 구조적 상태를 유지 $\Rightarrow$ OS에게 독립적인 논리적 CPU가 있는 것처럼 보임
	- 당연히 스케쥴링이 복잡하겠죠..
![](https://i.imgur.com/4O4tHKF.png)

- **Memory stall**
	- 프로세서가 메모리에 접근할때, 데이터가 가용해지기를 기다리며 많은 시간을 낭비함
	- 이유 : 캐시메모리에 없는 데이터여서 or 요즘 프로세서들은 메모리보다 속도가 빨라서
![|400](https://i.imgur.com/RJGaqhN.png)

- 이런 상황을 피하려면, 요즘 하드웨어들은 **multithreaded processing core**를 구현함
	- **둘 이상의 하드웨어 스레드를 한 코어에 할당**
	- 한 하드웨어 스레드가 메모리를 기다리며 **stall되면, 코어가 다른 스레드로 전환**됨

![](https://i.imgur.com/DmwMz5M.png)
- **Chip Multithreading(CMT)**
	![](https://i.imgur.com/TZOvsRc.png)

	- **OS의 관점**에서 보면,
	- 각각의 하드웨어 스레드는 구조적 상태(포인터,레지스터)를 유지함
	- OS는 (**하드웨어 스레드 : 소프트웨어 스레드를 실행하는 논리적인 CPU)** 로 간주하고 스케쥴링 함
	- 즉, **프로세서가 computing core 4개, 각 코어당 하드웨어 스레드 2개면**
	- OS의 관점에서는 **8개의 논리적 CPU**가 있는 거지.
	![|400](https://i.imgur.com/1YvtQby.png)
	

- **코어를 멀티스레드 하는 두가지 방법 (즉 언제 switch 할 것인가?)**
	![](https://i.imgur.com/DmwMz5M.png)
	- **Coarse-grained multithreading** : 처리에 시간이 오래 걸리는 이벤트가 나오면 스위치 
		- 스위칭 비용 높음
	- **Fine-grained multithreading** : 세밀한 단위(intstruction cycle)에서 스위치
		- 스위칭 비용 낮음

- 코어는 한번에 하나의 하드웨어 스레드만 실행함
- 캐시/파이프라인과 같은 물리적 코어 자원은 하드웨어 스레드 간 공유되어야 함
- **결과적으로, 멀티스레드, 멀티코어 프로세서는** 
	- **소프트웨어 스레드 선택 (하드웨아 스레드에서 실행될)**
	- **하드웨어 스레드 선택 (각 코어에서 실행될)**
- **이렇게 두 단계의 스케쥴링이 필요함**
- **1단계에서 OS의 스케쥴러가 자원 공유 정보를 안다면 효과적 스케쥴링 가능**
	- 같은 코어에 스케쥴 되면, 자원을 공유해서 느려질 수 있으니까
	- 자원 공유를 OS가 안다면, **공유하지 않도록 스케쥴**하겠지.

## 5.5.3 Load Balancing

- SMP 시스템에서는 ,멀티프로세서 이점을 최대한 뽑아먹으려고 부하를 균등하게 유지해야 함
- 그렇지 않으면, 하나 이상의 프로레서가 놀고 있을 때 다른 프로세서에 부하가 커짐 (레디큐가 CPU를 기다림..)
- **해결책) Load balancing**
	- **전체적으로 공평하게 load를 맞춰주는 거**
	- 각 프로세서별로 자신의 레디큐가 있으면 필요함
	- 공통 레디큐(AMP)면, 불필요하겠지


- **Load balancing을 위한 두가지 방법**
	- **Push migration**
		- 특정 task가 주기적으로 load 검사
		- misimbalance가 발견되면 스레드를 move/push
	- **Pull migration**
		- 노는(idle) processor가 바쁜(busy) processor로부터 waiting task를 pull(땡겨옴)
	- 위 두개는 상호배타적이지 않음, 병렬로 구현할 수 있음

- **비교) Balanced load (Load balancing이라는 다른 개념임)**
	- 단순히 모든 큐에 대충 **동일한 개수**의 스레드가 있도록 하는 거나
	- 모든 큐에 스레드 대충 **동일한 우선수위**가 분포하는 거

## 5.5.4 Processor Affinity (선호도)

- 스레드가 특정 프로세스에서 실행될 때 캐시 메모리는 어떻게 되는가?
	- **Warm cache**
		- 스레드가 가장 최근에 접근한 데이터가, 프로세서의 캐시를 채운다
		- 스레드에서의 연속적인 메모리 접근은 캐시 메모리에서 처리됨 (warm cache)
- 그럼 **Load balancing으로 migration이 발생**한다면? 
	- Migration **이전의 캐시 메모리는** **무효**가 됨
	- Migration **이후의 캐시가 채워**짐
	- 근데 이렇게 무효+다시 채우기가 비용을 많이 잡아먹어서
		- 대부분의 **SMP 기반 OS는 migrating을 기피**함
		- 그대신 , 같은 프로세스에 스레드를 올리려 함
		- **warh cache를 이용**하는 거지

- Affinity : 프로세스는 자기가 지금 돌고 있는 프로세스를 선호한다!


- 스레드 **큐의 구성이 선호도에 영향을** 미침
	- 만약 우리가 **common 레디 큐**를 쓴다면,
		- 스레드는 임의의 processor에서 실행 가능
		- 새로운 프로세스에서 스케쥴 된다면, 캐시가 새로 채워져야 함
	- **프로세서별 레디 큐를** 따로 쓴다면,
		- 스레드는 항상 같은 프로세서에서 실행 가능함
		- 이때 **warm cache**의 내용을 갖다 씀
		- 즉 이경우 **processor affinity**를 제공하는 거죠

- **Soft affinity**
	- OS는 프로세스가 동일 프로세스에서 실행되게 **노력은 함**
	- 근데 **load balancing** 때문에 다른 프로세서에게 이주하는건 못막음
- **Hard affinity**
	- system call을 써서 특정 프로세서에서 실행되도록 **보장함!**

- 메인 메모리 구조가 processor affinity에도 영향을 줌
	- NUMA-aware인 경우, 가까운 메모리에 할당한다.

## 5.5 Heterogeneous Multiprocessing

- Homogeneous : 모든 프로세서가 기능 면에서 동일 $\Rightarrow$ 스레드는 임의 코어에서 실행 가능
	- load balancing, processor affinity, NUMA 등에 따라 memory access time이 달라짐
- **Heterogeneous multiprocessing(HMP)**
	- 코어들이 기능적으로 다를 수 있음 (slow + fast)
	- 스레드가 특정 목적으로 특정 코어에서 실행될 수 있음
	- 왜 씀?
		- task의 특정 요구에 기반하여 특정 코어에 할당 $\Rightarrow$ 전력 소비를 더 잘 관리
		- slow+fast를 같이 쓰면
			- 고성능을 요구하지 않지만 오래 실행해야하는 (백그라운드) task를 little 코어에 할당
			- 고성능을 요구하지만 짧게 실행해야 하는 interactive task를 big 코어에 할당

<hr style=\"page-break-after: always;\">

# 5.6 Real-Time CPU scheduling

- Soft real-time OS
	- 중요한 실시간 프로세스가 언제 스케줄 되는지 보장 X
	- 중요할 수록 우선적으로 스케줄 되는것만 보장됨
- Hard real-time OS
	- 더 요구조건이 엄격
		- task는 반드시 deadline까지 서비스 되어야함
		- deadline 이후의 서비스 : 서비스 받지 않은 것과 동일

## 5.6.1 Minimizing Latency

- Real-time system은 event 위주로 돌아감
	- 시스템은 이벤트가 발생하는 걸 기다리고 있음
	- 이벤트는 소프트웨어, 하드웨어에서 모두 발생 가능
	- 일단 이벤트가 발생하면, 최대한 빨리 응답/서비스 해야 함

- Event Latency : (이벤트 발생)~(서비스 될때) 까지 걸린 시간

- 이벤트마다 당연히 latency가 다르겠죠
- real-time system에 영향을 주는 두 종류의 latency
	- **Interrupt Latency : (인터럽트 발생)~(인터럽트 서비스 루틴 시작) 까지 걸린 시간**
		- 실시간 task가 즉시 수행되도록 interrupt latency를 최소화 해야 함
		- 근데 단순 최소화는 또 아님. 엄격한 요구 사항을 충족해야 함
		- kernel data가 업데이트 되는 동안 인터럽트가 disable 되므로, 이 시간을 줄여야 함
		- 즉, **인터럽트는 아주 짧은 시간 동안만 disable 되어야 함** 
	- **Dispatch Latency : Dispacther가 한 프로세스를 멈추고 다른 프로세스를 시작하기 위해 필요한 시간**
		- 즉 **Context-Switch 타임**!!
		- Real time OS는 이것도 최소화 해야 함
		- 최소화 하는 가장 좋은 방법 : **선점형 커널 사용!**
		![](https://i.imgur.com/Agw2dHS.png)
		- Conflict phase : 선점/자원해제
		- Dispatch phase : CPU 스케쥴링

## 5.6.2 Priority-Based scheduling

- 지금까지 배운 우선순위 스케쥴링과 똑같음
- 근데 Hard real-time은 task가 deadline 요구 내에 서비스되어야 하므로,
- 이것만으론 안되고 추가적인 기법이 필요함!!! (아래부터 중요)

## <span style="color:rgb(192, 0, 0)">5.6.3 Rate-Monotonic (RM) Scheduling</span>

- **Real time Process의 특성**
	- **일정 간격으로 CPU를 요구**한다 (periodic)
	![](https://i.imgur.com/H5l0mC9.png)
	- **프로세스는 스케줄러에게 자신의 deadline 요구사항을 알려야한다**
	- **승인-제어 알고리즘을 사용해서, 스케쥴러는 둘 중 하나를 수행**
		- 프로세스가 시간 내에 완료 가능함을 **보장 $\Rightarrow$ 허락**
		- **보장 못함 $\Rightarrow$ 거절** 

- <span style="color:rgb(192, 0, 0)">Periodic task를 고정 Priority Policy + 선점으로 스케쥴링</span> 
- Period의 역수(1/p)가 곧 priority!
- Period가 짧을수록 우선순위가 높아
	- CPU를 자주 요청하는 놈일수록 우선순위가 높음
- 가정!
	- Processing time = 각 CPU burst time
	- 프로세스가 CPU를 차지한 시간 = 프로세스의 CPU burst duration
- 예제
![](https://i.imgur.com/JiIWwcH.png)

- deadline : 다음 주기의 시작(현재 주기의 끝)까지 CPU burst가 완료되어야 함
- 먼저, 각 deadline을 만족하도록 task를 스케줄링 할 수 있는지 확인
- CPU utilization(이용률) = period / processing time 의 비율
![](https://i.imgur.com/s3eCW51.png)

- 100퍼센트보다 적으니, 두 프로세스의 데드라인을 만족! 25%가 남도록 스케쥴링 가능
- P1은 1/50 = 0.02, P2는 1/100 = 0.01이므로 P1이 우선순위 더 높음
- **하지만 P2가 먼저 시작한다면?**
	- P2가 t=35에 끝나고
	- P1이 t=35+20=55에 끝날거 아님
	- 근데 P1의 첫번째 deadline은 50이었어. 이러면 나가리야!!!
	![](https://i.imgur.com/H8aFbNI.png)

- **P1이 먼저 시작한다면**
	![](https://i.imgur.com/Z2KzOUf.png)

	- P1이 t=20에 끝남
	- P2는 t=20~50까지. (P1 새 주기 선점) 어차피 75%가 최적이고, P2 데드라인은 100까지니까..
	- P1이 t=50~70까지 함. P1의 두번째 데드라인 (100) 안에 들어옴
	- P2가 t=70~75로 100 안에 P2의 할당량(35)를 끝냈다.
	- P1이 다시 스케쥴되는 t=100까지는 IDLE 상태를 유지!

- **RM 스케쥴링은 optimal임이 보장.**
	- 어떤 프로세스 집합을 **이 알고리즘으로 스케줄링 하는게 불가능하다면,**
	- **static priority를 쓰는 어떤 알고리즘이든 스케줄링 불가능하단 뜻**


- 예제) 아예 스케줄이 안 되는 process 집합도 있음
	![](https://i.imgur.com/ZVXYdXo.png)
	- P1에 높은 우선순위를 줌.
	- 25/50 = 0.5, 35/80 = 0.437이니까 total utilization은 94%임
	- 6%가 남을 것으로 보이지만..
	- P1이 0~25, 그다음에 P2 25~50, P1이 50~75, P2 75~85, 데드라인 못 지킴!!!!
	- 이러면 다른 알고리즘으로도 못 한다는 얘기
	![](https://i.imgur.com/uWQVkEm.png)

- Optimal임에도, RM scheduling은 한계가 있음
	- Utilization은 이론적 상한이 있어서, CPU 자원을 항상 최대화할 순 없음.
	- $N$개의 프로세스에 대한 worst-case CPU utilization은 $N(2^{1/N}-1)$ 임.
	- 즉 1개일땐 100%, 2개일땐 83%, 무한대에서 69% 선이라는 얘기야.
	- 첫번째 예제에서는 total utilization이 75%<83%였으니, 데드라인을 지키는 것이 보장되지만
	- 방금 예제에서는 94%>84%(이론적 상한)보다 크니까, 데드라인을 지키는걸 보장 못하는 거임.

## <span style="color:rgb(0, 112, 192)">5.6.4 Earliest-Deadline-First (EDF) Scheduling</span>

- EDF 스케쥴링은 데드라인에 따라 우선순위를 "동적"으로 할당함.
- **데드라인이 일찍일 수록, 우선순위 높아짐**
- process는 실행가능한 상태가 되면, system에 deadline을 알려야 함
	- 즉 deadline에 따라 선점이 가능
- **우선순위는 새로 실행가능해진 process의 deadline을 반영해서 조정이 됨!**
- 이 경우, 방금 실패했던 케이스에서 스케쥴링 가능 (우연임 ㅋ)
	![](https://i.imgur.com/ZVXYdXo.png)
	![](https://i.imgur.com/n41AbYW.png)
	- RM의 경우 P1의 deadline에서 P1이 선점을 해버렸지만,
	- EDF의 경우, 이미 P1은 첫번째 데드라인(50)안에 지켰고, 다음 데드라인은 100.
	- 근데 P2는 데드라인이 80이니까 가장 데드라인이 빨리 오는 P2에 우선순위가 높아짐.
	- 그래서 P2는 선점되지 않고, t=25~60동안 실행이 됨.
	- 그 다음엔 P1이 t=60~85동안 실행되어 자신의 두번째 데드라인 이전에 끝냄.
	- P2가 이어받아서, t=85부터 시작하지만, P2의 다음 데드라인은 160임
	- 이 경우 P1의 다음 데드라인이 100이기 때문에, P1이 우선순위를 차지함
	- t=100에서 P1이 선점을 함. 다시 P1이 100~125동안 돌림. 
	- 이제 P2가 이어받아서 125~145동안 함.  이제 P1과 P2 모두 당장 급한 데드라인은 모두 마친 상태.
	- 이제 P1으로 넘어가는데, P1의 다음 주기는 150이 되어서야 시작하므로 150까지는 IDLE 상태임~

- RM 알고리즘과 달리, **EDF**는 **프로세스가 periodic하거나 CPU burst가 상수일 필요가 없음**
- 단지 실행 가능해졌을 때 **스케쥴러에게 deadline을 알리기만 하면 됨**
- **이론적으로, (각 프로세스의 deadline 요구를 만족 / CPU 이용률 100%) 모두 가능**
- 근데 현실적으로는 context switch, interrput때문에 100%는 불가

## 5.6.5 Proportional Share Scheduling

- 모든 application들이 CPU 사용시간 $T$ shares를 할당 받음.
- 하나의 application이 $N$ shares를 받았다면, 전체 processor time의 $N/T$만큼을 할당받은거임.
- 예제) Process 3개, T =100
	- A : 50 share $\Rightarrow$ 50%  
	- B : 15 share $\Rightarrow$ 15%  
	- C : 20 share $\Rightarrow$ 20%  
- 이 스케줄러는 할당량을 보장하기 위해, admission-control policy와 함께 동작해야 됨
	- 보당할 수 있을때만 accept한다는 얘기임
	- 가용한 share가 있을 때만, 특정 share을 요구하는 걸 허용함
	- 저 위의 상황에서 D 가 30 share 달라고 하면 deny, 10 share이면 줌 ㅋ

- POSIX는 우선순위 기반 realtime 용으로 $\mathtt{SCHED\_FIFO,SCHED\_RR}$ 제공. (먼지 알지?)
	- 우선순위 같으면 FIFO는 순서대로, RR는 timeslice.

<hr style=\"page-break-after: always;\">
# 5.8 Algorithm Evaluation

- 스케줄링 알고리즘을 어떤 방식으로 고를 것인가?

1. 선택 기준을 정의한다 (utilization, throughput, turnaround, waiting, response..)
2. 선택 기준 하에서 알고리즘을 평가한다.

## 5.8.1 Deterministic Modeling


- 분석적 평가 유형 중 하나
- 이론적으로는 계산할때마다 똑같은 값이 나와야됨
- 미리 정의된 특정 워크로드를 이용해 성능을 정의함

![](https://i.imgur.com/lEzRbOw.png)

- 이런 식으로요


- 간단, 빠름
- 정확한 값이 나옴 (정확한 값이 입력될 때만 적용되기도 함)
- 보통 설명/예시에 쓰임..
- 동일 프로그램을 반복실행/요구사항 정확히 측정이 가능하면, 이 방법 써봄직함


## 5.8.3 Simulation

- 더 정확한 평가를 하기 위해, 시뮬레이션을 생각해볼 수 있다
- **시뮬레이션 : 컴퓨터 시스템에 대한 모델을 프로그래밍**
	- 시스템 주요 요소를 소프트웨어 자료 구조로 표현함
	- 시뮬레이터는 시간을 표현하는 변수가 있음
	- 시간값이 증가함에 따라 시스템의 활동을 반영하기 위해 상태를 변경
	- 알고리즘 성능을 나타내는 통계들이 수집/출력됨

- 시뮬레이션 구동을 위한 데이터 생성
	- Random number generator (분포는 수학/경험적으로)
		- 실제 시스템에서는 연속적으로 발생하는 이벤트들 사이의 관계로 인해 부정확함 ㅠㅠ 
	- 해결책) Trace file 이용하기
		- 실제 시스템을 모니터링 / 이벤트 순서를 기록함
		- 이걸 시뮬레이션에 쓰면, 입력에 대한 정확한 결과물이 나오겠지
-
- 단점) 비싸.. 시간도 많이 걸려ㅠㅠ
	- 디테일한 시뮬레이션일수록 정확한 결과가 나오겠지만.. 시간 많이 잡아먹겠지ㅠㅠ
	- trace file은 공간도 많이 잡아먹어ㅠㅠ
	- 디자인, 코딩, 디버깅이 귀찮음 ㅠㅠ

## 5.8.4 Implementation

- 심지어 시뮬레이션 조차 정확도에 한계가 있잖아
- 알고리즘을 평가하는 **정확하면서도 유일한 방법은 ,코드 짜고 구현해서, OS에 집어넣고 어케 돌아가는지 보는거야**
- 단점) 코딩, 변경, 테스트에 비용 듬, 적용환경이 변화할수도 있음.. ㅠㅠ